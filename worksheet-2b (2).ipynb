{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9075432,"sourceType":"datasetVersion","datasetId":5474739}],"dockerImageVersionId":30749,"isInternetEnabled":false,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2024-09-02T19:17:46.041193Z","iopub.execute_input":"2024-09-02T19:17:46.043139Z","iopub.status.idle":"2024-09-02T19:17:46.08443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Overfitting and Regularization\n\nIn the realm of predictive modeling, the pursuit of creating a model that perfectly fits the training data can inadvertently lead to a phenomenon known as overfitting. Overfitting occurs when a model becomes excessively complex, capturing not only the genuine patterns within the data but also the noise and random fluctuations present in the training set. \n\nThe hyper-adaptation to the training data renders the model less capable of generalizing to new, unseen data, as it effectively memorizes the training examples rather than discerning meaningful relationships. As a result, an overfitted model may exhibit impressive performance on the training data but performs poorly when faced with real-world scenarios. The delicate balance between capturing essential patterns and avoiding the trap of overfitting underscores the importance of techniques like regularization, which aim to ensure model generalization by restraining excessive complexity.\n\nThere are three Regularization techniques we will be dealing with, all of which use the idea of penalizing terms to tackle overfitting.\n\nBut before we go any further, let's have a look at the data.","metadata":{}},{"cell_type":"markdown","source":"**Data Dictionary**\n\n* Product_ID:- Unique identifier for each electronic device. <br>\n* Product_Weight:- Weight of the device  <br>\n* Energy_Efficiency:- Energy efficiency rating (e.g., \"Energy Efficient\", \"Standard\"). <br>\n* Visibility_Index:- The % of the total display area of all products in a store allocated to the particular product <br>\n* Product_Category:- Category of the electronic device (e.g., \"Laptops\", \"Smartphones\"). <br>\n* Product_Price:- Price of the device in dollars<br>\n* Store_ID:- Unique identifier for the store.<br>\n* Store_Established_Year:- Number of years since the store was established.<br>\n* Store_Size:- Size classification of the store (e.g., \"Small\", \"Medium\").<br>\n* Location_Type:- The type of city in which the store is located<br>\n* Store_Type:- Type of store (e.g., \"Electronics Store Type1\").<br>\n* Sales_Performance:- Sales of the electronic device in the particular store. This is the outcome variable to be predicted.\n","metadata":{}},{"cell_type":"markdown","source":"## Visualizing the data\n\nLet's visualize this all in the form of a Data Frame","metadata":{}},{"cell_type":"code","source":"data <- read.csv(\"/kaggle/input/2b-data/2b-data.csv\")\nhead(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T10:49:56.029917Z","iopub.execute_input":"2024-09-08T10:49:56.032187Z","iopub.status.idle":"2024-09-08T10:49:56.257255Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":"<table class=\"dataframe\">\n<caption>A data.frame: 6 Ã— 12</caption>\n<thead>\n\t<tr><th></th><th scope=col>Product_ID</th><th scope=col>Product_Weight</th><th scope=col>Energy_Efficiency</th><th scope=col>Visibility_Index</th><th scope=col>Product_Category</th><th scope=col>Product_Price</th><th scope=col>Store_ID</th><th scope=col>Store_Established_Year</th><th scope=col>Store_Size</th><th scope=col>Location_Type</th><th scope=col>Store_Type</th><th scope=col>Sales_Performance</th></tr>\n\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n</thead>\n<tbody>\n\t<tr><th scope=row>1</th><td>FDA15</td><td> 9.300</td><td>Energy Efficient</td><td>0.01604730</td><td>Laptops      </td><td>249.8092</td><td>OUT049</td><td>14</td><td>Medium</td><td>Tier 1</td><td>Electronics Store Type1 </td><td>3735.1380</td></tr>\n\t<tr><th scope=row>2</th><td>DRC01</td><td> 5.920</td><td>Standard        </td><td>0.01927822</td><td>Smartphones  </td><td> 48.2692</td><td>OUT018</td><td> 4</td><td>Medium</td><td>Tier 3</td><td>Electronics Store Type2 </td><td> 443.4228</td></tr>\n\t<tr><th scope=row>3</th><td>FDN15</td><td>17.500</td><td>Energy Efficient</td><td>0.01676007</td><td>Tablets      </td><td>141.6180</td><td>OUT049</td><td>14</td><td>Medium</td><td>Tier 1</td><td>Electronics Store Type1 </td><td>2097.2700</td></tr>\n\t<tr><th scope=row>4</th><td>FDX07</td><td>19.200</td><td>Standard        </td><td>0.06613203</td><td>Monitors     </td><td>182.0950</td><td>OUT010</td><td>15</td><td>Small </td><td>Tier 3</td><td>Retail Electronics Store</td><td> 732.3800</td></tr>\n\t<tr><th scope=row>5</th><td>NCD19</td><td> 8.930</td><td>Energy Efficient</td><td>0.06613203</td><td>Refrigerators</td><td> 53.8614</td><td>OUT013</td><td>26</td><td>High  </td><td>Tier 3</td><td>Electronics Store Type1 </td><td> 994.7052</td></tr>\n\t<tr><th scope=row>6</th><td>FDP36</td><td>10.395</td><td>Standard        </td><td>0.06613203</td><td>Microwaves   </td><td> 51.4008</td><td>OUT018</td><td> 4</td><td>Medium</td><td>Tier 3</td><td>Electronics Store Type2 </td><td> 556.6088</td></tr>\n</tbody>\n</table>\n","text/markdown":"\nA data.frame: 6 Ã— 12\n\n| <!--/--> | Product_ID &lt;chr&gt; | Product_Weight &lt;dbl&gt; | Energy_Efficiency &lt;chr&gt; | Visibility_Index &lt;dbl&gt; | Product_Category &lt;chr&gt; | Product_Price &lt;dbl&gt; | Store_ID &lt;chr&gt; | Store_Established_Year &lt;int&gt; | Store_Size &lt;chr&gt; | Location_Type &lt;chr&gt; | Store_Type &lt;chr&gt; | Sales_Performance &lt;dbl&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 1 | FDA15 |  9.300 | Energy Efficient | 0.01604730 | Laptops       | 249.8092 | OUT049 | 14 | Medium | Tier 1 | Electronics Store Type1  | 3735.1380 |\n| 2 | DRC01 |  5.920 | Standard         | 0.01927822 | Smartphones   |  48.2692 | OUT018 |  4 | Medium | Tier 3 | Electronics Store Type2  |  443.4228 |\n| 3 | FDN15 | 17.500 | Energy Efficient | 0.01676007 | Tablets       | 141.6180 | OUT049 | 14 | Medium | Tier 1 | Electronics Store Type1  | 2097.2700 |\n| 4 | FDX07 | 19.200 | Standard         | 0.06613203 | Monitors      | 182.0950 | OUT010 | 15 | Small  | Tier 3 | Retail Electronics Store |  732.3800 |\n| 5 | NCD19 |  8.930 | Energy Efficient | 0.06613203 | Refrigerators |  53.8614 | OUT013 | 26 | High   | Tier 3 | Electronics Store Type1  |  994.7052 |\n| 6 | FDP36 | 10.395 | Standard         | 0.06613203 | Microwaves    |  51.4008 | OUT018 |  4 | Medium | Tier 3 | Electronics Store Type2  |  556.6088 |\n\n","text/latex":"A data.frame: 6 Ã— 12\n\\begin{tabular}{r|llllllllllll}\n  & Product\\_ID & Product\\_Weight & Energy\\_Efficiency & Visibility\\_Index & Product\\_Category & Product\\_Price & Store\\_ID & Store\\_Established\\_Year & Store\\_Size & Location\\_Type & Store\\_Type & Sales\\_Performance\\\\\n  & <chr> & <dbl> & <chr> & <dbl> & <chr> & <dbl> & <chr> & <int> & <chr> & <chr> & <chr> & <dbl>\\\\\n\\hline\n\t1 & FDA15 &  9.300 & Energy Efficient & 0.01604730 & Laptops       & 249.8092 & OUT049 & 14 & Medium & Tier 1 & Electronics Store Type1  & 3735.1380\\\\\n\t2 & DRC01 &  5.920 & Standard         & 0.01927822 & Smartphones   &  48.2692 & OUT018 &  4 & Medium & Tier 3 & Electronics Store Type2  &  443.4228\\\\\n\t3 & FDN15 & 17.500 & Energy Efficient & 0.01676007 & Tablets       & 141.6180 & OUT049 & 14 & Medium & Tier 1 & Electronics Store Type1  & 2097.2700\\\\\n\t4 & FDX07 & 19.200 & Standard         & 0.06613203 & Monitors      & 182.0950 & OUT010 & 15 & Small  & Tier 3 & Retail Electronics Store &  732.3800\\\\\n\t5 & NCD19 &  8.930 & Energy Efficient & 0.06613203 & Refrigerators &  53.8614 & OUT013 & 26 & High   & Tier 3 & Electronics Store Type1  &  994.7052\\\\\n\t6 & FDP36 & 10.395 & Standard         & 0.06613203 & Microwaves    &  51.4008 & OUT018 &  4 & Medium & Tier 3 & Electronics Store Type2  &  556.6088\\\\\n\\end{tabular}\n","text/plain":"  Product_ID Product_Weight Energy_Efficiency Visibility_Index Product_Category\n1 FDA15       9.300         Energy Efficient  0.01604730       Laptops         \n2 DRC01       5.920         Standard          0.01927822       Smartphones     \n3 FDN15      17.500         Energy Efficient  0.01676007       Tablets         \n4 FDX07      19.200         Standard          0.06613203       Monitors        \n5 NCD19       8.930         Energy Efficient  0.06613203       Refrigerators   \n6 FDP36      10.395         Standard          0.06613203       Microwaves      \n  Product_Price Store_ID Store_Established_Year Store_Size Location_Type\n1 249.8092      OUT049   14                     Medium     Tier 1       \n2  48.2692      OUT018    4                     Medium     Tier 3       \n3 141.6180      OUT049   14                     Medium     Tier 1       \n4 182.0950      OUT010   15                     Small      Tier 3       \n5  53.8614      OUT013   26                     High       Tier 3       \n6  51.4008      OUT018    4                     Medium     Tier 3       \n  Store_Type               Sales_Performance\n1 Electronics Store Type1  3735.1380        \n2 Electronics Store Type2   443.4228        \n3 Electronics Store Type1  2097.2700        \n4 Retail Electronics Store  732.3800        \n5 Electronics Store Type1   994.7052        \n6 Electronics Store Type2   556.6088        "},"metadata":{}}]},{"cell_type":"code","source":"colnames(data)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:17:40.548149Z","iopub.execute_input":"2024-09-08T11:17:40.550047Z","iopub.status.idle":"2024-09-08T11:17:40.570084Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/html":"<style>\n.list-inline {list-style: none; margin:0; padding: 0}\n.list-inline>li {display: inline-block}\n.list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n</style>\n<ol class=list-inline><li>'Product_ID'</li><li>'Product_Weight'</li><li>'Energy_Efficiency'</li><li>'Visibility_Index'</li><li>'Product_Category'</li><li>'Product_Price'</li><li>'Store_ID'</li><li>'Store_Established_Year'</li><li>'Store_Size'</li><li>'Location_Type'</li><li>'Store_Type'</li><li>'Sales_Performance'</li></ol>\n","text/markdown":"1. 'Product_ID'\n2. 'Product_Weight'\n3. 'Energy_Efficiency'\n4. 'Visibility_Index'\n5. 'Product_Category'\n6. 'Product_Price'\n7. 'Store_ID'\n8. 'Store_Established_Year'\n9. 'Store_Size'\n10. 'Location_Type'\n11. 'Store_Type'\n12. 'Sales_Performance'\n\n\n","text/latex":"\\begin{enumerate*}\n\\item 'Product\\_ID'\n\\item 'Product\\_Weight'\n\\item 'Energy\\_Efficiency'\n\\item 'Visibility\\_Index'\n\\item 'Product\\_Category'\n\\item 'Product\\_Price'\n\\item 'Store\\_ID'\n\\item 'Store\\_Established\\_Year'\n\\item 'Store\\_Size'\n\\item 'Location\\_Type'\n\\item 'Store\\_Type'\n\\item 'Sales\\_Performance'\n\\end{enumerate*}\n","text/plain":" [1] \"Product_ID\"             \"Product_Weight\"         \"Energy_Efficiency\"     \n [4] \"Visibility_Index\"       \"Product_Category\"       \"Product_Price\"         \n [7] \"Store_ID\"               \"Store_Established_Year\" \"Store_Size\"            \n[10] \"Location_Type\"          \"Store_Type\"             \"Sales_Performance\"     "},"metadata":{}}]},{"cell_type":"markdown","source":"## Ridge Regression\nRidge regression is a linear regression technique that incorporates L2 regularization to address issues in predictive modeling (overfitting, multi-colinearity etc).\nLinear regression, aims to minimize the sum of squared residuals whereas Ridge regression introduces a penalty term proportional to the square of the magnitude of the coefficients. This penalty, controlled by a hyperparameter (often denoted as lambda), discourages large coefficient values, effectively constraining the modelâ€™s complexity, enhancing its stability and generalization performance.\n","metadata":{}},{"cell_type":"markdown","source":"**1)** Perform Ridge Regression on the training data and compare the predictions with the test data to check for the fit of the model. (Hint: Use the glmnet library) (2 marks) <br>\nYou can split the dataset into 70% train and 30% test.  ","metadata":{}},{"cell_type":"code","source":"data1 <- data[,c('Product_Weight','Energy_Efficiency',\n                 'Visibility_Index','Product_Category','Product_Price',\n                 'Store_ID','Store_Established_Year','Store_Size',\n                 'Location_Type','Store_Type','Sales_Performance')]","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:22:59.741725Z","iopub.execute_input":"2024-09-08T11:22:59.743409Z","iopub.status.idle":"2024-09-08T11:22:59.758341Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Write your code here\n\nlibrary(glmnet)\n\nset.seed(123)\n\n# Split the data into 70% train and 30% test\ntrain_indices <- sample(1:nrow(data1), size = 0.7 * nrow(data1))\ntrain_data <- data1[train_indices, ]\ntest_data <- data1[-train_indices, ]\n\n# Separate features (X) and the target variable (y)\nx_train <- as.matrix(train_data[, -ncol(train_data)])  # All columns except the last one\ny_train <- train_data[, ncol(train_data)]  # The last column (target)\n\nx_test <- as.matrix(test_data[, -ncol(test_data)])  # All columns except the last one\ny_test <- test_data[, ncol(test_data)]  # The last column (target)\n\nrm_1 <- glmnet(x_train, y_train, alpha = 0, lambda = 0)\ncoef(rm_1)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:28:30.713838Z","iopub.execute_input":"2024-09-08T11:28:30.715714Z","iopub.status.idle":"2024-09-08T11:28:30.810146Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Warning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"11 x 1 sparse Matrix of class \"dgCMatrix\"\n                                s0\n(Intercept)              117.16744\nProduct_Weight            -1.95295\nEnergy_Efficiency          .      \nVisibility_Index       -4804.99897\nProduct_Category           .      \nProduct_Price             15.71742\nStore_ID                   .      \nStore_Established_Year    13.91777\nStore_Size                 .      \nLocation_Type              .      \nStore_Type                 .      "},"metadata":{}}]},{"cell_type":"markdown","source":"**2)** Is it possible for you to somehow conduct hyperparameter tuning and find the best lambda value for the Ridge Regression model? (Hint: use the cv.glmnet function)   (1 mark)","metadata":{}},{"cell_type":"code","source":"cv_model <- cv.glmnet(x_train, y_train, alpha = 0)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda <- cv_model$lambda.min\nprint(best_lambda)\n\nbest_model <- glmnet(x_train, y_train, alpha = 0, lambda = best_lambda)\ncoef(best_model)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:25:21.771765Z","iopub.execute_input":"2024-09-08T11:25:21.773805Z","iopub.status.idle":"2024-09-08T11:25:22.154419Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Warning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\nWarning message in cbind2(1, newx) %*% nbeta:\nâ€œNAs introduced by coercionâ€\n","output_type":"stream"},{"name":"stdout","text":"[1] 97.67512\n","output_type":"stream"},{"name":"stderr","text":"Warning message in storage.mode(xd) <- \"double\":\nâ€œNAs introduced by coercionâ€\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"11 x 1 sparse Matrix of class \"dgCMatrix\"\n                                 s0\n(Intercept)              224.445976\nProduct_Weight            -1.546382\nEnergy_Efficiency          .       \nVisibility_Index       -4539.368712\nProduct_Category           .       \nProduct_Price             14.877235\nStore_ID                   .       \nStore_Established_Year    13.106636\nStore_Size                 .       \nLocation_Type              .       \nStore_Type                 .       "},"metadata":{}}]},{"cell_type":"markdown","source":"**3)** With the optimal lambda, build the model again and print the coefficients of the various dependent variables. What can you comment about the relationship between lambda and the strength of regularization?        (2 marks)\n","metadata":{}},{"cell_type":"code","source":"plot(rm_1, xvar = \"lambda\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:38:00.612045Z","iopub.execute_input":"2024-09-08T11:38:00.614737Z","iopub.status.idle":"2024-09-08T11:38:00.694504Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Warning message in min(x):\nâ€œno non-missing arguments to min; returning Infâ€\nWarning message in max(x):\nâ€œno non-missing arguments to max; returning -Infâ€\n","output_type":"stream"},{"ename":"ERROR","evalue":"Error in plot.window(...): need finite 'xlim' values\n","traceback":["Error in plot.window(...): need finite 'xlim' values\nTraceback:\n","1. plot(rm_1, xvar = \"lambda\")","2. plot.glmnet(rm_1, xvar = \"lambda\")","3. plotCoef(x$beta, lambda = x$lambda, df = x$df, dev = x$dev.ratio, \n .     label = label, xvar = xvar, ...)","4. matplot(index, t(beta), lty = 1, xlab = xlab, ylab = ylab, type = \"l\", \n .     ...)","5. plot(x[, 1L], y[, 1L], type = type[1L], xlab = xlab, ylab = ylab, \n .     xlim = xlim, ylim = ylim, lty = lty[1L], lwd = lwd[1L], lend = lend[1L], \n .     pch = pch[1L], col = col[1L], cex = cex[1L], bg = bg[1L], \n .     log = log, ...)","6. plot.default(x[, 1L], y[, 1L], type = type[1L], xlab = xlab, \n .     ylab = ylab, xlim = xlim, ylim = ylim, lty = lty[1L], lwd = lwd[1L], \n .     lend = lend[1L], pch = pch[1L], col = col[1L], cex = cex[1L], \n .     bg = bg[1L], log = log, ...)","7. localWindow(xlim, ylim, log, asp, ...)","8. plot.window(...)"],"output_type":"error"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAABmJLR0QA/wD/AP+gvaeTAAAP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"},"metadata":{"image/png":{"width":420,"height":420}}}]},{"cell_type":"markdown","source":"Coefficients have been printed above \n\n\nLambda which provides the best trade-off between bias (underfitting) and variance (overfitting). Optimal Lambda is chosen to minimize prediction error on unseen data.\n\nSmall ðœ†\nLambda = Weak regularization, more complexity, risk of overfitting.\nLarge ðœ†\nLambda = Strong regularization, less complexity, risk of underfitting.\nOptimal ðœ†\nÎ» = Balanced regularization, better generalization on new data.","metadata":{}},{"cell_type":"markdown","source":"## Lasso Regression\n\nLasso regression is similar to Ridge Regression except that instead of L2 regularization, it employs L1 regularization to address the very same issues that Ridge Regression addresses. \n\nThere are however, a couple of differences between the two. The first and most obvious being that since Lasso Regression implements L1 regularization, the penalty term in this case is proportional to the absolute value of the coefficient. \n\nAnother point to note is that unlike its Ridge counterpart, Lasso Regression can push some coefficients to exactly 0. This effectively drops the feature from the predictive model (Similar to how we drop values through visual analysis). Lasso Regression can thus be used effectively for Feature Selection as well.","metadata":{}},{"cell_type":"markdown","source":"**1)** Write code to build a Lasso Regression model similar to how you built the Ridge Regression model. This time incorporate hyperparameter tuning right away. So first print the optimal lambda value.                  (2 marks)","metadata":{}},{"cell_type":"markdown","source":"**2)** Display the coefficients of all the variables. Do you notice some variables being dropped out? Which ones are they?   (1 mark)","metadata":{}},{"cell_type":"markdown","source":"## Elastic Net Regression\n\nElastic Net regression, an advanced form of linear regression, combines the benefits of L1 (Lasso) and L2 (Ridge) regularization methods. By integrating both penalty terms, Elastic Net overcomes the limitations of each, offering resilience against multicollinearity, aiding feature selection, and preventing overfitting. \n\nThis approach makes Elastic Net a very versatile approach for achieving accurate and efficient models by finding a middle ground between dropping parameters and retaining important predictors. ","metadata":{}},{"cell_type":"markdown","source":"**1)** Build your Elastic Net Regression model incorporating all the steps we previously followed for ridge and lasso regression. (Play around with the alpha value and find out how it affects the model)            (2 marks)\n","metadata":{}},{"cell_type":"code","source":"## Write your code here","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:17:46.189762Z","iopub.execute_input":"2024-09-02T19:17:46.191333Z","iopub.status.idle":"2024-09-02T19:17:46.201805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Identify and list the variables that have been dropped in the Elastic Net model. How does this compare to the variables dropped in the Lasso model? <br>\nWhat does that tell you about the number of hyperparameters in Elastic Net Regression compared to the other two models?","metadata":{}}]}